{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_4_Assignment_4_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLVhCarMwG70",
        "colab_type": "text"
      },
      "source": [
        "### **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXJ1XpSwdvR",
        "colab_type": "text"
      },
      "source": [
        "Install and import all the necessary libraries for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNMrFD-ZBwK",
        "colab_type": "code",
        "outputId": "2afac348-8e4c-4c5d-9eb1-3186a490a430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-rc0 in /usr/local/lib/python3.6/dist-packages (2.0.0rc0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.14.0.dev2019080601)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a20190807,>=1.15.0a20190806 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0a20190806)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-rc0) (41.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (0.15.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGgAUOKwsWA",
        "colab_type": "text"
      },
      "source": [
        "### **Importing the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOe2azQOdmND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_dataset = load_boston()\n",
        "\n",
        "data_X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "data_Y = pd.DataFrame(boston_dataset.target, columns=[\"target\"])\n",
        "data = pd.concat([data_X, data_Y], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gD5esSxfxjs",
        "colab_type": "code",
        "outputId": "621b03fb-670b-4c1b-9392-2a86236ffea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "train, val = train_test_split(train, test_size=0.2, random_state=1)\n",
        "print(len(train), \"train examples\")\n",
        "print(len(val), \"validation examples\")\n",
        "print(len(test), \"test examples\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "323 train examples\n",
            "81 validation examples\n",
            "102 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZTeC55HxDeT",
        "colab_type": "text"
      },
      "source": [
        "Converting the Pandas DataFrames into Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF4GRPPLdTIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdZy7p3AaTRT",
        "colab_type": "code",
        "outputId": "e24b78e4-f0bd-4099-9d51-84e228d166ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, True, batch_size)\n",
        "val_ds = df_to_dataset(val, False, batch_size)\n",
        "test_ds = df_to_dataset(test, False, batch_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVKpasevkSFR",
        "colab_type": "code",
        "outputId": "acec566b-c916-47c0-b107-1c9e28a1a29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
              "       'PTRATIO', 'B', 'LSTAT', 'target'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63KuTr4sxMl6",
        "colab_type": "text"
      },
      "source": [
        "### Defining Feature Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "380jHjPokFUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define feature_columns as a list of features using functions from tf.feature_column\n",
        "\n",
        "feature_columns = []\n",
        "for each in ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX','PTRATIO', 'B', 'LSTAT']:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(key=each))\n",
        "\n",
        "# RAD = tf.feature_column.numeric_column(key='RAD')\n",
        "\n",
        "# # bucketized cols\n",
        "# rad_buckets = tf.feature_column.bucketized_column(RAD,boundaries=[2,5])\n",
        "# feature_columns.append(rad_buckets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXFrKIdklEl6",
        "colab_type": "code",
        "outputId": "92ae6907-6283-4ef9-b7ba-232dbabf338d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "feature_columns"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NumericColumn(key='CRIM', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='ZN', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='INDUS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='CHAS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='NOX', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='RM', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='AGE', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='DIS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='RAD', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='TAX', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='PTRATIO', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='B', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='LSTAT', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVCMrdMxVB5",
        "colab_type": "text"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAc9LpVzqql9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6B9FgRyyGXe",
        "colab_type": "text"
      },
      "source": [
        "Model should contain following layers:\n",
        "\n",
        "```\n",
        "feature_layer\n",
        "\n",
        "Dense(1, activation=None)\n",
        "```\n",
        "\n",
        "Use 'Adam' optimizer\n",
        "\n",
        "Use 'mse' as your loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZInuZ8D0xsu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build and compile your model in this cell.\n",
        "model = tf.keras.Sequential([feature_layer,\n",
        "                             tf.keras.layers.Dense(1,activation=None)])\n",
        "model.compile(loss=tf.keras.losses.MeanAbsolutePercentageError(),optimizer='adam',metrics=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igdzl3wasRo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46d2a5c1-8ffa-4125-8399-7174b06a2312"
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 854.7614 - mse: 31554.8398 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 792.7499 - mse: 28237.4277 - val_loss: 744.3431 - val_mse: 24969.1875\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 739.6645 - mse: 25152.2578 - val_loss: 692.8337 - val_mse: 22233.7852\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 686.5256 - mse: 22302.1797 - val_loss: 642.1577 - val_mse: 19662.7051\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 634.6122 - mse: 19614.3398 - val_loss: 594.7363 - val_mse: 17305.7910\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 585.7468 - mse: 17174.4746 - val_loss: 551.3943 - val_mse: 15178.5449\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 544.9502 - mse: 14997.1475 - val_loss: 512.6785 - val_mse: 13251.0107\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 501.9298 - mse: 13023.2402 - val_loss: 475.3403 - val_mse: 11548.3857\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 462.5146 - mse: 11284.0840 - val_loss: 438.6239 - val_mse: 10021.3750\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 426.2692 - mse: 9777.3350 - val_loss: 403.8653 - val_mse: 8707.0850\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 390.1654 - mse: 8486.1201 - val_loss: 371.6108 - val_mse: 7583.4946\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 355.8367 - mse: 7286.6694 - val_loss: 342.9583 - val_mse: 6492.9790\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 323.0826 - mse: 6226.8120 - val_loss: 318.1155 - val_mse: 5569.5479\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 292.6077 - mse: 5328.6021 - val_loss: 301.8302 - val_mse: 4867.7563\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 277.0946 - mse: 4697.1333 - val_loss: 292.5369 - val_mse: 4360.9258\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 268.0591 - mse: 4238.0493 - val_loss: 285.6155 - val_mse: 4033.3604\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 263.2024 - mse: 3966.4961 - val_loss: 279.8135 - val_mse: 3829.6130\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 258.4291 - mse: 3777.6348 - val_loss: 274.2286 - val_mse: 3660.5598\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 252.9961 - mse: 3594.2551 - val_loss: 268.3999 - val_mse: 3479.2314\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 247.8116 - mse: 3408.4795 - val_loss: 262.3708 - val_mse: 3295.9036\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 243.7703 - mse: 3221.5928 - val_loss: 256.2073 - val_mse: 3118.0647\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 238.8367 - mse: 3050.4487 - val_loss: 250.2017 - val_mse: 2958.8499\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 233.3231 - mse: 2912.2673 - val_loss: 244.6450 - val_mse: 2832.0886\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 228.6789 - mse: 2770.2822 - val_loss: 238.5261 - val_mse: 2677.4880\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 220.1027 - mse: 2609.7761 - val_loss: 232.2974 - val_mse: 2520.6694\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 217.5322 - mse: 2485.4817 - val_loss: 226.4463 - val_mse: 2419.0020\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 211.7822 - mse: 2362.9321 - val_loss: 220.1407 - val_mse: 2282.9419\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 205.4621 - mse: 2261.2280 - val_loss: 215.3937 - val_mse: 2219.6003\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 198.6058 - mse: 2197.5891 - val_loss: 210.4333 - val_mse: 2153.5256\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 195.9912 - mse: 2122.1726 - val_loss: 204.3254 - val_mse: 2052.3818\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 189.6155 - mse: 2011.3268 - val_loss: 198.0039 - val_mse: 1931.8633\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 184.5600 - mse: 1901.2152 - val_loss: 192.2314 - val_mse: 1845.7440\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 178.1448 - mse: 1818.7454 - val_loss: 186.5715 - val_mse: 1759.4166\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 173.2287 - mse: 1715.9941 - val_loss: 180.1169 - val_mse: 1641.7645\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 167.6736 - mse: 1594.1332 - val_loss: 173.3949 - val_mse: 1515.0081\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 162.1514 - mse: 1477.7131 - val_loss: 166.9661 - val_mse: 1409.7267\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 155.9595 - mse: 1383.3713 - val_loss: 160.9824 - val_mse: 1322.3755\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 149.4578 - mse: 1283.4369 - val_loss: 154.2801 - val_mse: 1219.8007\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 144.1484 - mse: 1188.4288 - val_loss: 147.4656 - val_mse: 1128.7719\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 138.3502 - mse: 1085.1099 - val_loss: 140.2604 - val_mse: 1019.0075\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 131.9175 - mse: 987.9011 - val_loss: 133.6038 - val_mse: 931.8466\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 126.0332 - mse: 892.2540 - val_loss: 126.4352 - val_mse: 831.8029\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 120.2804 - mse: 801.1504 - val_loss: 119.6034 - val_mse: 750.2614\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 113.9599 - mse: 728.0792 - val_loss: 112.9699 - val_mse: 679.5856\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 107.9083 - mse: 640.7051 - val_loss: 105.2504 - val_mse: 587.8088\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 101.4443 - mse: 566.3044 - val_loss: 98.6127 - val_mse: 527.3038\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 95.4595 - mse: 496.7498 - val_loss: 90.9796 - val_mse: 452.6401\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 89.1935 - mse: 426.8556 - val_loss: 83.3833 - val_mse: 388.0329\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 82.7971 - mse: 364.7995 - val_loss: 75.9658 - val_mse: 332.8082\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 76.6860 - mse: 316.8198 - val_loss: 69.5750 - val_mse: 290.5103\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 70.9702 - mse: 276.0135 - val_loss: 63.2518 - val_mse: 253.9795\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 65.3496 - mse: 239.9483 - val_loss: 57.4091 - val_mse: 222.9084\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 59.4525 - mse: 213.1620 - val_loss: 52.3281 - val_mse: 202.6652\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 53.8124 - mse: 196.8130 - val_loss: 47.9585 - val_mse: 193.6868\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 48.7426 - mse: 186.0513 - val_loss: 43.3020 - val_mse: 176.3305\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 44.9068 - mse: 157.5298 - val_loss: 38.6271 - val_mse: 145.9778\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 41.0239 - mse: 133.6457 - val_loss: 35.7665 - val_mse: 130.4314\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 38.8158 - mse: 120.5298 - val_loss: 34.4934 - val_mse: 122.1110\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 38.5311 - mse: 116.3110 - val_loss: 34.5696 - val_mse: 122.9672\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 37.5983 - mse: 111.8490 - val_loss: 33.6036 - val_mse: 112.8382\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 36.8194 - mse: 100.2733 - val_loss: 33.2993 - val_mse: 105.1243\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 36.5186 - mse: 95.7135 - val_loss: 33.0399 - val_mse: 102.9080\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 35.8594 - mse: 95.6271 - val_loss: 32.6917 - val_mse: 106.3630\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 35.3857 - mse: 96.9537 - val_loss: 32.4133 - val_mse: 104.9882\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 35.1585 - mse: 94.7586 - val_loss: 32.0846 - val_mse: 101.9509\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 34.7610 - mse: 92.1632 - val_loss: 31.7548 - val_mse: 100.2659\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 34.1909 - mse: 92.5525 - val_loss: 31.4932 - val_mse: 100.8897\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 33.7446 - mse: 90.2649 - val_loss: 31.1850 - val_mse: 98.3546\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 33.4550 - mse: 93.8275 - val_loss: 32.0103 - val_mse: 108.7235\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 33.3745 - mse: 98.7852 - val_loss: 31.5673 - val_mse: 105.5801\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 32.9879 - mse: 95.2716 - val_loss: 30.8571 - val_mse: 99.1521\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 32.4755 - mse: 89.8508 - val_loss: 30.6709 - val_mse: 97.2510\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 31.3159 - mse: 86.9513 - val_loss: 30.5305 - val_mse: 96.9509\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 32.6278 - mse: 102.6837 - val_loss: 32.8840 - val_mse: 117.4514\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 33.2851 - mse: 108.1799 - val_loss: 31.0897 - val_mse: 104.1577\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 31.5478 - mse: 90.4532 - val_loss: 30.1130 - val_mse: 93.5356\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 31.1977 - mse: 86.0616 - val_loss: 29.9834 - val_mse: 93.5559\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 30.8813 - mse: 85.8838 - val_loss: 29.8265 - val_mse: 94.5883\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 31.2387 - mse: 92.3519 - val_loss: 30.0918 - val_mse: 98.0445\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 30.5584 - mse: 86.2048 - val_loss: 29.2307 - val_mse: 89.1081\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 30.5450 - mse: 80.4898 - val_loss: 28.9030 - val_mse: 88.7649\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 30.7360 - mse: 89.1346 - val_loss: 29.5207 - val_mse: 98.9846\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 30.0391 - mse: 89.9347 - val_loss: 28.5817 - val_mse: 91.6991\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 30.0238 - mse: 87.7452 - val_loss: 28.5441 - val_mse: 92.9254\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 29.5441 - mse: 82.9954 - val_loss: 28.0757 - val_mse: 85.2504\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 29.7592 - mse: 78.8104 - val_loss: 27.7980 - val_mse: 85.6897\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 29.4349 - mse: 79.1375 - val_loss: 27.5474 - val_mse: 86.9399\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 29.2046 - mse: 82.6746 - val_loss: 27.6165 - val_mse: 90.4658\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 29.3538 - mse: 86.8288 - val_loss: 27.3826 - val_mse: 90.0059\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 28.7623 - mse: 82.5753 - val_loss: 26.7879 - val_mse: 83.9383\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 28.3955 - mse: 76.4616 - val_loss: 26.6043 - val_mse: 83.4341\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 28.4927 - mse: 80.7990 - val_loss: 26.7963 - val_mse: 88.2553\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 28.4797 - mse: 82.1387 - val_loss: 26.3639 - val_mse: 85.8940\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 28.1176 - mse: 80.3474 - val_loss: 26.1049 - val_mse: 83.1871\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 28.1620 - mse: 76.2310 - val_loss: 25.9356 - val_mse: 81.9653\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 27.6167 - mse: 80.4236 - val_loss: 25.6835 - val_mse: 83.7632\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 28.0623 - mse: 74.3493 - val_loss: 25.3016 - val_mse: 77.8913\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 27.5989 - mse: 74.4428 - val_loss: 25.0697 - val_mse: 81.6399\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 27.5098 - mse: 78.1708 - val_loss: 24.9587 - val_mse: 83.5216\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 27.4651 - mse: 79.0311 - val_loss: 24.6913 - val_mse: 78.8927\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 27.5888 - mse: 71.4285 - val_loss: 24.6153 - val_mse: 76.2990\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26.9703 - mse: 75.2699 - val_loss: 24.6993 - val_mse: 83.4690\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 27.1083 - mse: 78.7948 - val_loss: 24.3901 - val_mse: 80.5513\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26.9211 - mse: 78.4714 - val_loss: 24.3934 - val_mse: 82.2642\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26.6734 - mse: 78.0153 - val_loss: 24.1537 - val_mse: 80.5174\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26.4619 - mse: 77.5341 - val_loss: 23.8804 - val_mse: 79.5047\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26.4045 - mse: 74.9057 - val_loss: 23.7269 - val_mse: 78.3551\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26.4541 - mse: 75.2001 - val_loss: 23.6651 - val_mse: 79.6649\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26.2594 - mse: 74.0081 - val_loss: 23.5221 - val_mse: 78.3678\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26.3704 - mse: 77.3195 - val_loss: 23.4414 - val_mse: 78.3379\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26.1405 - mse: 72.3686 - val_loss: 23.2837 - val_mse: 74.6831\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.8130 - mse: 70.2819 - val_loss: 23.1116 - val_mse: 75.9221\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26.0142 - mse: 76.4929 - val_loss: 23.6104 - val_mse: 82.5941\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26.1888 - mse: 78.0539 - val_loss: 23.0726 - val_mse: 78.4055\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.8085 - mse: 75.1694 - val_loss: 22.9381 - val_mse: 77.1835\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.7475 - mse: 72.5623 - val_loss: 22.8063 - val_mse: 74.7284\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.7075 - mse: 69.5383 - val_loss: 22.6733 - val_mse: 74.2437\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.1845 - mse: 72.2419 - val_loss: 22.8338 - val_mse: 78.8070\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.8022 - mse: 77.7403 - val_loss: 22.7960 - val_mse: 78.6774\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.7442 - mse: 70.3341 - val_loss: 22.4109 - val_mse: 70.3256\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.4340 - mse: 70.1791 - val_loss: 22.5141 - val_mse: 76.9992\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.4951 - mse: 75.6303 - val_loss: 22.4759 - val_mse: 76.4514\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.3637 - mse: 70.0017 - val_loss: 22.2322 - val_mse: 71.7471\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.0060 - mse: 70.4271 - val_loss: 22.4382 - val_mse: 76.7645\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.0605 - mse: 74.8389 - val_loss: 22.1202 - val_mse: 74.9302\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 25.0400 - mse: 68.8200 - val_loss: 21.8809 - val_mse: 71.0478\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24.9688 - mse: 68.4540 - val_loss: 21.8915 - val_mse: 74.3631\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.8101 - mse: 72.5633 - val_loss: 21.8491 - val_mse: 74.4534\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.9008 - mse: 68.3047 - val_loss: 21.6656 - val_mse: 71.2360\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.9877 - mse: 66.9353 - val_loss: 21.5920 - val_mse: 71.8392\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24.7523 - mse: 72.7985 - val_loss: 21.9449 - val_mse: 76.4090\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.5737 - mse: 69.7596 - val_loss: 21.4387 - val_mse: 70.3415\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.5268 - mse: 68.9916 - val_loss: 21.5627 - val_mse: 74.0627\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.6683 - mse: 69.7627 - val_loss: 21.3109 - val_mse: 70.1072\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.2576 - mse: 67.2611 - val_loss: 21.2771 - val_mse: 72.0105\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24.4555 - mse: 70.3470 - val_loss: 21.2994 - val_mse: 72.6869\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.4677 - mse: 68.6805 - val_loss: 21.1464 - val_mse: 71.4563\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 24.4768 - mse: 71.7494 - val_loss: 21.1089 - val_mse: 70.7649\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.0676 - mse: 65.8290 - val_loss: 21.0532 - val_mse: 67.4377\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24.2795 - mse: 65.9097 - val_loss: 20.9234 - val_mse: 69.1517\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24.1204 - mse: 63.0224 - val_loss: 20.8596 - val_mse: 67.6091\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.9166 - mse: 68.6120 - val_loss: 21.2099 - val_mse: 74.6494\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.7600 - mse: 69.0923 - val_loss: 20.6759 - val_mse: 70.2751\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.8829 - mse: 68.9327 - val_loss: 20.7408 - val_mse: 71.5478\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.7037 - mse: 66.9043 - val_loss: 20.5462 - val_mse: 68.2595\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.7086 - mse: 65.3808 - val_loss: 20.7465 - val_mse: 72.3796\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.9797 - mse: 71.9193 - val_loss: 20.5441 - val_mse: 70.9322\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.8387 - mse: 62.3863 - val_loss: 20.7259 - val_mse: 64.8793\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.8649 - mse: 64.9895 - val_loss: 20.4730 - val_mse: 71.0579\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.4184 - mse: 66.9255 - val_loss: 20.2403 - val_mse: 69.1065\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.3399 - mse: 67.3081 - val_loss: 20.4839 - val_mse: 71.9032\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.1972 - mse: 65.9056 - val_loss: 20.1253 - val_mse: 67.4902\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.5719 - mse: 62.9719 - val_loss: 20.3052 - val_mse: 71.2090\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.5090 - mse: 75.9280 - val_loss: 21.2479 - val_mse: 73.9593\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.3614 - mse: 67.3666 - val_loss: 19.8992 - val_mse: 66.3787\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.1394 - mse: 63.1215 - val_loss: 19.8559 - val_mse: 66.8337\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.4551 - mse: 68.7798 - val_loss: 20.0441 - val_mse: 69.3422\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.9435 - mse: 63.3863 - val_loss: 20.2064 - val_mse: 64.0255\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.3448 - mse: 61.1378 - val_loss: 19.7669 - val_mse: 67.9394\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.7851 - mse: 66.2106 - val_loss: 20.4087 - val_mse: 72.6802\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.9628 - mse: 65.1795 - val_loss: 19.9603 - val_mse: 68.1241\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.1372 - mse: 67.8571 - val_loss: 20.3925 - val_mse: 72.2856\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.1496 - mse: 70.0220 - val_loss: 19.7150 - val_mse: 68.4810\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.7522 - mse: 65.5293 - val_loss: 19.6389 - val_mse: 66.8067\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.0101 - mse: 61.8801 - val_loss: 20.2766 - val_mse: 65.5338\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.2070 - mse: 62.5793 - val_loss: 20.4276 - val_mse: 69.6671\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.9900 - mse: 67.8812 - val_loss: 20.2952 - val_mse: 70.3564\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.0169 - mse: 62.8344 - val_loss: 20.0853 - val_mse: 65.4495\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.4445 - mse: 70.1594 - val_loss: 20.8825 - val_mse: 73.3307\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.4327 - mse: 62.1401 - val_loss: 20.3440 - val_mse: 61.9274\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.7357 - mse: 61.9387 - val_loss: 19.6843 - val_mse: 68.5924\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.3350 - mse: 62.8125 - val_loss: 20.0329 - val_mse: 65.9766\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.5730 - mse: 63.0449 - val_loss: 20.5351 - val_mse: 72.2830\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.8178 - mse: 73.7010 - val_loss: 20.4957 - val_mse: 72.4647\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.6599 - mse: 63.8948 - val_loss: 20.1727 - val_mse: 67.1307\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.9051 - mse: 71.5289 - val_loss: 20.5864 - val_mse: 72.8653\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.5013 - mse: 63.5020 - val_loss: 20.0009 - val_mse: 63.8932\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.6339 - mse: 64.1769 - val_loss: 19.9802 - val_mse: 69.9096\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.3802 - mse: 64.2907 - val_loss: 19.5477 - val_mse: 65.4448\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.4869 - mse: 65.4456 - val_loss: 19.9356 - val_mse: 69.6900\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.2114 - mse: 62.9276 - val_loss: 19.3462 - val_mse: 65.1753\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.1165 - mse: 64.3737 - val_loss: 20.2086 - val_mse: 69.5632\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.7057 - mse: 67.4980 - val_loss: 19.1254 - val_mse: 63.8924\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.0500 - mse: 61.8617 - val_loss: 19.3886 - val_mse: 64.8927\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.1317 - mse: 63.4635 - val_loss: 19.2226 - val_mse: 64.4273\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.1702 - mse: 63.0933 - val_loss: 19.6562 - val_mse: 67.9691\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 22.2415 - mse: 67.8071 - val_loss: 19.4163 - val_mse: 66.6360\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.0525 - mse: 60.6087 - val_loss: 19.2699 - val_mse: 63.3232\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21.8071 - mse: 62.9400 - val_loss: 19.4959 - val_mse: 66.8741\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.0093 - mse: 61.9873 - val_loss: 19.1135 - val_mse: 62.5284\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.0325 - mse: 60.7304 - val_loss: 19.2121 - val_mse: 64.1172\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21.8882 - mse: 61.2191 - val_loss: 19.2767 - val_mse: 64.5726\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21.7546 - mse: 63.1801 - val_loss: 19.1279 - val_mse: 62.8266\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.0224 - mse: 59.3225 - val_loss: 19.1623 - val_mse: 63.0446\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21.8187 - mse: 61.3305 - val_loss: 19.1769 - val_mse: 62.9776\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21.6077 - mse: 60.2055 - val_loss: 19.3648 - val_mse: 64.9538\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.5351 - mse: 68.2864 - val_loss: 19.2770 - val_mse: 64.8260\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21.7303 - mse: 59.5457 - val_loss: 19.1556 - val_mse: 62.5081\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21.7828 - mse: 62.1243 - val_loss: 19.4653 - val_mse: 66.2059\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21.9012 - mse: 58.7640 - val_loss: 19.6453 - val_mse: 59.9656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6d2e3172e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFu2k4J_spfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "42390ba1-86b5-4241-9e5e-3804b3ed82b4"
      },
      "source": [
        "#  1 model without bucktized column and MSE\n",
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 59.0249 - mse: 58.5053\n",
            "Mean Squared Error - Test Data 58.505283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PuTG6MetTOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1dac455e-0081-4511-faf5-372ca885ff2b"
      },
      "source": [
        "#   1 model with bucktized column and MSE\n",
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 54.7149 - mse: 57.2912\n",
            "Mean Squared Error - Test Data 57.29124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGRR7HfFzJ-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "03ef2d0e-e0c8-417e-f3cc-4f5782f0cf7b"
      },
      "source": [
        "#  1 model without bucktized column and with MAPE\n",
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 22.9422 - mse: 73.5606\n",
            "Mean Squared Error - Test Data 73.56064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxcsATp3tUFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}